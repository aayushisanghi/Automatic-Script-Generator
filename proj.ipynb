{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open('Downloads/text.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "\n",
    "#print(vocab)\n",
    "#print(vocab_to_int)\n",
    "#print(int_to_vocab)\n",
    "\n",
    "#encoded contains the entire text, encoded character-wise. Example: MONICA: 29 56 ...etc where 29 is M and 56 is O\n",
    "#print(encoded)\n",
    "\n",
    "def get_batches(arr, batch_size, n_steps):\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr)//chars_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y_temp = arr[:, n+1:n+n_steps+1]\n",
    "        \n",
    "        # For the very last batch, y will be one character short at the end of \n",
    "        # the sequences which breaks things. To get around this, I'll make an \n",
    "        # array of the appropriate size first, of all zeros, then add the targets.\n",
    "        # This will introduce a small artifact in the last batch, but it won't matter.\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        yield x, y\n",
    "\n",
    "\n",
    "#batches = get_batches(encoded, 10, 50)\n",
    "#x,y = next(batches)\n",
    "\n",
    "#print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout'''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers'''\n",
    "    \n",
    "    #Build the LSTM Cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/150...  Training Step: 50...  Training loss: 3.3616...  0.0608 sec/batch\n",
      "Epoch: 6/150...  Training Step: 100...  Training loss: 3.2991...  0.0583 sec/batch\n",
      "Epoch: 8/150...  Training Step: 150...  Training loss: 3.2797...  0.0594 sec/batch\n",
      "Epoch: 11/150...  Training Step: 200...  Training loss: 3.0136...  0.0658 sec/batch\n",
      "Epoch: 14/150...  Training Step: 250...  Training loss: 2.7779...  0.0700 sec/batch\n",
      "Epoch: 16/150...  Training Step: 300...  Training loss: 2.6750...  0.0687 sec/batch\n",
      "Epoch: 19/150...  Training Step: 350...  Training loss: 2.5539...  0.0628 sec/batch\n",
      "Epoch: 22/150...  Training Step: 400...  Training loss: 2.5633...  0.0630 sec/batch\n",
      "Epoch: 24/150...  Training Step: 450...  Training loss: 2.4176...  0.0642 sec/batch\n",
      "Epoch: 27/150...  Training Step: 500...  Training loss: 2.3568...  0.0722 sec/batch\n",
      "Epoch: 29/150...  Training Step: 550...  Training loss: 2.3047...  0.0718 sec/batch\n",
      "Epoch: 32/150...  Training Step: 600...  Training loss: 2.2801...  0.0622 sec/batch\n",
      "Epoch: 35/150...  Training Step: 650...  Training loss: 2.1883...  0.0650 sec/batch\n",
      "Epoch: 37/150...  Training Step: 700...  Training loss: 2.1971...  0.0642 sec/batch\n",
      "Epoch: 40/150...  Training Step: 750...  Training loss: 2.2335...  0.0641 sec/batch\n",
      "Epoch: 43/150...  Training Step: 800...  Training loss: 2.1422...  0.0640 sec/batch\n",
      "Epoch: 45/150...  Training Step: 850...  Training loss: 2.0845...  0.0632 sec/batch\n",
      "Epoch: 48/150...  Training Step: 900...  Training loss: 2.0680...  0.0632 sec/batch\n",
      "Epoch: 50/150...  Training Step: 950...  Training loss: 2.1793...  0.0634 sec/batch\n",
      "Epoch: 53/150...  Training Step: 1000...  Training loss: 2.0621...  0.0662 sec/batch\n",
      "Epoch: 56/150...  Training Step: 1050...  Training loss: 2.0781...  0.0632 sec/batch\n",
      "Epoch: 58/150...  Training Step: 1100...  Training loss: 2.0570...  0.0655 sec/batch\n",
      "Epoch: 61/150...  Training Step: 1150...  Training loss: 1.9802...  0.0653 sec/batch\n",
      "Epoch: 64/150...  Training Step: 1200...  Training loss: 1.9599...  0.0689 sec/batch\n",
      "Epoch: 66/150...  Training Step: 1250...  Training loss: 1.9906...  0.0665 sec/batch\n",
      "Epoch: 69/150...  Training Step: 1300...  Training loss: 1.8952...  0.0635 sec/batch\n",
      "Epoch: 72/150...  Training Step: 1350...  Training loss: 1.9986...  0.0647 sec/batch\n",
      "Epoch: 74/150...  Training Step: 1400...  Training loss: 1.9104...  0.0650 sec/batch\n",
      "Epoch: 77/150...  Training Step: 1450...  Training loss: 1.8617...  0.0670 sec/batch\n",
      "Epoch: 79/150...  Training Step: 1500...  Training loss: 1.8679...  0.0667 sec/batch\n",
      "Epoch: 82/150...  Training Step: 1550...  Training loss: 1.8147...  0.0660 sec/batch\n",
      "Epoch: 85/150...  Training Step: 1600...  Training loss: 1.7747...  0.0685 sec/batch\n",
      "Epoch: 87/150...  Training Step: 1650...  Training loss: 1.8090...  0.0643 sec/batch\n",
      "Epoch: 90/150...  Training Step: 1700...  Training loss: 1.8633...  0.0676 sec/batch\n",
      "Epoch: 93/150...  Training Step: 1750...  Training loss: 1.7532...  0.0657 sec/batch\n",
      "Epoch: 95/150...  Training Step: 1800...  Training loss: 1.7756...  0.0661 sec/batch\n",
      "Epoch: 98/150...  Training Step: 1850...  Training loss: 1.6971...  0.0645 sec/batch\n",
      "Epoch: 100/150...  Training Step: 1900...  Training loss: 1.8452...  0.0667 sec/batch\n",
      "Epoch: 103/150...  Training Step: 1950...  Training loss: 1.7133...  0.0667 sec/batch\n",
      "Epoch: 106/150...  Training Step: 2000...  Training loss: 1.7657...  0.0622 sec/batch\n",
      "Epoch: 108/150...  Training Step: 2050...  Training loss: 1.7220...  0.0695 sec/batch\n",
      "Epoch: 111/150...  Training Step: 2100...  Training loss: 1.6869...  0.0639 sec/batch\n",
      "Epoch: 114/150...  Training Step: 2150...  Training loss: 1.6806...  0.0631 sec/batch\n",
      "Epoch: 116/150...  Training Step: 2200...  Training loss: 1.7269...  0.0637 sec/batch\n",
      "Epoch: 119/150...  Training Step: 2250...  Training loss: 1.6210...  0.0779 sec/batch\n",
      "Epoch: 122/150...  Training Step: 2300...  Training loss: 1.7055...  0.0642 sec/batch\n",
      "Epoch: 124/150...  Training Step: 2350...  Training loss: 1.6808...  0.0638 sec/batch\n",
      "Epoch: 127/150...  Training Step: 2400...  Training loss: 1.6239...  0.0651 sec/batch\n",
      "Epoch: 129/150...  Training Step: 2450...  Training loss: 1.6120...  0.0645 sec/batch\n",
      "Epoch: 132/150...  Training Step: 2500...  Training loss: 1.5939...  0.0663 sec/batch\n",
      "Epoch: 135/150...  Training Step: 2550...  Training loss: 1.5662...  0.0649 sec/batch\n",
      "Epoch: 137/150...  Training Step: 2600...  Training loss: 1.5602...  0.0612 sec/batch\n",
      "Epoch: 140/150...  Training Step: 2650...  Training loss: 1.6399...  0.0620 sec/batch\n",
      "Epoch: 143/150...  Training Step: 2700...  Training loss: 1.5427...  0.0650 sec/batch\n",
      "Epoch: 145/150...  Training Step: 2750...  Training loss: 1.5678...  0.0620 sec/batch\n",
      "Epoch: 148/150...  Training Step: 2800...  Training loss: 1.4840...  0.0640 sec/batch\n",
      "Epoch: 150/150...  Training Step: 2850...  Training loss: 1.6222...  0.0647 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "# Print losses every N interations\n",
    "print_every_n = 50\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=64, num_steps=50,\n",
    "                lstm_size=128, num_layers=2, \n",
    "                learning_rate=0.001)\n",
    "batch_size=64\n",
    "num_steps=50\n",
    "lstm_size=128\n",
    "num_layers=2\n",
    "learning_rate=0.001\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.6,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i2850_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2850_l128.ckpt\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i2850_l128.ckpt'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i2850_l128.ckpt\n",
      "Farne of where and to so tood ale ant of a gust a have tho go are the gace the sares it and down and sopry, that's sain warse abing of the geys hic, his... I'r is a wanna go that is. It's stand that he sayt about this stoprs.)\n",
      "\n",
      "Ross: I just want? (He the go and the chone, a store surse)) He may, in I dive in a this thing? Would tald yow' thous tat ot thook.\n",
      "\n",
      "Joey: Okan, and what's are and thing it aro the guts ale home the boom the asl and, the leses he seal.)\n",
      "\n",
      "Monica: (can his to sing oft the.) Yeah, I'm a coftere to that hadrout a bay...\n",
      "\n",
      "Rachel: (entiliging and tare as a stors and a phistens.\n",
      "\n",
      "Monica: Well, you know that's a going and this as a hava somesseen.\n",
      "\n",
      "Paul: (To Minniss tho a grong and and somene to geans.)\n",
      "\n",
      "Ross: You conk in, the goy, to courd and you cume to dinn of her somed that whim thing a shas the comenat, and Rachll sist some. Whore you have to the tomething, and I'll get abing this that a got hear.)\n",
      "\n",
      "Joey: (sarts to the goongeren.. (tres ano stires him ald and and dayser, and there's on mat or a lot to mowe an a there. I'r hever wish and storres and about of it and so comentere.)\n",
      "\n",
      "Monica: (to Chonine) The big a this are a shead in that? I wess time with this all of thriens, thous wat arouge.\n",
      "\n",
      "Monica: I- I was hen and you whis ofters, it sounteds it soot here.)\n",
      "\n",
      "Monisa: What. I wenn't be her shoming and thas so that what what they we kit to hall, and, you're hers?\n",
      "\n",
      "Joey: (mandsting of to are and soonessed) (So hactict) I'm that is to louks and and thirs if thing the sites and do and the shall. (To Ross) What to to to to sour the change to.\n",
      "\n",
      "Monica: (mo there of takes.)\n",
      "\n",
      "Phoebe: Weal, you get tomine!\n",
      "\n",
      "Rachel: I know it the gendorse.]\n",
      "\n",
      "Monica: Oh, no, I here whe hand that with you a leaving of her.\n",
      "\n",
      "Chandler: What?\n",
      "\n",
      "Joey: (serts a dreat the carts and thene have with to leap, and the singantes im shepas, to car is the change.)\n",
      "\n",
      "Chandler: Oh, she stalk of ous a dutire.\n",
      "\n",
      "Phoebe: (park aclous to thit tacc.\n",
      "\n",
      "Jaey: You gon't guts a thing on sim. (They all stense all this. (Ho cand) I detting it sat how...\n",
      "\n",
      "Monica: Oh, yeah.. talk. Yiuh had, you shout! Y'know, torey?\n",
      "\n",
      "Ross: I'm sepry have a starise ho sorryong in and at toring.]\n",
      "\n",
      "Ross: (to shure) All and your oftoding and that?\n",
      "\n",
      "Joey: Oh! (Hises a sant the choend) (Souss the chall) Rachel everyenes a thir to hour tall and the showed her thing the that?\n",
      "\n",
      "Monica: Oh all a loutling ig.\n",
      "\n",
      "Chandler: Oh, I'r thy gets.\n",
      "\n",
      "Rachel: You'd big tated your aring the hand? You wind the selly hat a stor whim. Yeah, it's now the thand and ther that to the somresten and thas walks, they he deit on, whe wan a stare the gonna what was gottad to goang and and welling of the thand at a han is is to have and some with surling about a lead. (The prose the stear and the boocs how.)\n",
      "\n",
      "Rachel: Oh was-\n",
      "\n",
      "Monisa: Yes, ald the hers, I dean't think, it's not?\n",
      "\n",
      "Ross: What way.\n",
      "\n",
      "Ross: (somesseon sorros and toring and a think and that tomereiter.]\n",
      "\n",
      "Ross: Yeah, abark that's way with it they was hen the titting of the concamare tas the right. (To Choncler)) Oh Go, whel wend you sore, I know wat a think to and you were there thing womes of the chunted seet.\n",
      "\n",
      "Monica: Oh you here.\n",
      "\n",
      "Phoebe: Oh, I know that hight on me an it?\n",
      "\n",
      "Monica: Wes yuus. (Thangss the carstor)\n",
      "\n",
      "Ross: How what and she simt nack in the cant.)\n",
      "\n",
      "Ross: I know a latelde.\n",
      "\n",
      "Ross: Y'know, I hand think that, I well the carentime a there.\n",
      "\n",
      "Ross: You cen the carry?!\n",
      "\n",
      "Phoebe: Okay. We here was we kinna be for the witt the chitis? (The courss and and Rachel sheo ant a pore her hardss are funthone are and deann gring to han extire to car to the thome hing the crobe.)\n",
      "\n",
      "Joey: I went out to thise. The kang on to cauris in of a pors hourts) I knaw, it sere.\n",
      "\n",
      "Rachel: Oh, I wannite about a talkes it what was that wo did ta me for shime.\n",
      "\n",
      "Ross: Yeah, well, I know I was to have to halr in tho to got thas goend they a part at and stacts thit here the sher waschent, have and she cants.\n",
      "\n",
      "Ross: Okay.\n",
      "\n",
      "Ross: (camered.)\n",
      "\n",
      "Ross: I meen.\n",
      "\n",
      "Rachel: Oh, not mean any yer wat the shoor hon all all give wat wither are? I know a gotta have that have watting?\n",
      "\n",
      "Ross: Okay. (To Ross) It's to han abare.\n",
      "\n",
      "Monica: What? Well, it's sory that with in!\n",
      "\n",
      "Ross: (sarts the caulsing and Ressing and that stith out it takes the corttor and Chandlar and Carol it that they shears it to shing ond are she lass fact.. (Susts the coucksore to hore to all that and then are so shere, hut's time, are the stoniss of the comenand is thas hander and the pant and geat trish the comeshom the chalding him on a canes.)\n",
      "\n",
      "Ross: Oh... sey he way have timing is on to change the sappe so shoor to show and and taking a gitt a tod a tom it this abing, y'know. I don't give to that that a wombe ary. What what wank to me say thing!\n",
      "\n",
      "Paul: You were the harl to seon and tilk of what.\n",
      "\n",
      "Rachel: Oh. Okay, and you seen what's tore to to...\n",
      "\n",
      "Mandac: Is hand wers.)\n",
      "\n",
      "Rachel: (surting.]\n",
      "\n",
      "Ross: Okay, tell, in you all this?\n",
      "\n",
      "Rachel: I'm tond thing to so gey, and to to shaus, if in a sorradins of to thine whine so tare gonsa back to her a soor.\n",
      "\n",
      "Rachel: I- I mone wans thire sorond, are the garinged that was a thoubs.\n",
      "\n",
      "Chandler: Oh Go, God Gud, I when there with you have hight.  I'm to go it.\n",
      "\n",
      "Ross: Okay? We aren't wan to he stapse the gust?.]\n",
      "\n",
      "Monica: Y'm with you ceme the guy, a lave of all?\n",
      "\n",
      "Monica: I dester to me talk, you here women to tard ato how stack. I kind me.. I dester he wanting of this whowe if to to toer and dos this a gatting are and thes ano thas that? (Ho gusts in the hand a sottis.)\n",
      "\n",
      "Phoebe: Well, you gotta got in the serts on mart, and thes tore to like the sating of the shooke.)\n",
      "\n",
      "Chandler: What?\n",
      "\n",
      "Phaebe:\n",
      "\n",
      "Chandler: I was getters abeun't spean the goys her somesel the cartom that.\n",
      "\n",
      "Caloler: Well, I do surtere have to that a luchster, a crisents on the chungerss of.)\n",
      "\n",
      "Ross: What tours with that, I'm tareit a to tath.\n",
      "\n",
      "Carrer: So tasone, I well the getters to ham horetes of tare a lats far it was the grest the somoush to her a shane. Wallss tominith.]\n",
      "\n",
      "Ross: Well, that wert to think.\n",
      "\n",
      "Rachel: Yeah!\n",
      "\n",
      "Ross: (corfting out... (She somessers.)\n",
      "\n",
      "Chandler: Oh, y'krow, you got a look heme on mad times the goon, and you was.)\n",
      "\n",
      "Phoebe: Well, that's to here?\n",
      "\n",
      "Joey: What. The gons the soun it and alant it the cented..\n",
      "\n",
      "Monica: (suts him) Hey, and a liges to whal is?\n",
      "\n",
      "Ross: I don't sand is it in! Whe con.\n",
      "\n",
      "Rachel: What's to to thing of was having to the caness it the sore and and then cheed herper stirl a the sing and diming tones and you stor the sating, in tre the stor.\n",
      "\n",
      "Ross: (sants and Ross spachers out the gais ano sorright tate that his.)\n",
      "\n",
      "Ross: You here is to look of a dray.\n",
      "\n",
      "Racall: Oh!\n",
      "\n",
      "Rachel: I'm thanks a tall do to tart a what.\n",
      "\n",
      "Chandler: (marts to hour her.)\n",
      "\n",
      "Ross: Yeah, you was if I hat a tonint.)\n",
      "\n",
      "Ross: What this seed is there and your hiss and the lest and the bombone so tho gond.)\n",
      "\n",
      "Rachel: Ooh, stalliget wam with thim and there's ont prose, how's this to doon thas wanns and shers,. (Susss and this and to he souch and ther hard harcarts.)! Hi, you somenone the bit and they and you shar a stours.\n",
      "\n",
      "Monica: You know that's geand tomone will shoush?\n",
      "\n",
      "Rachel: I want in hing tattim. You're sord this all and and. (So and and ase and some to some to the sorrets and thand they go as it wome here.\n",
      "\n",
      "Rachel: I ming?\n",
      "\n",
      "Rachel: (theres are sofks.)\n",
      "\n",
      "Monica: I don't don't want that to to to can wo have this are this aros and sting the soments her the sofrette sintandy storss.\n",
      "\n",
      "Monica: (to Ress and the gits and go gitter it and stopts it sorf and thouss sor gortan to at to a last that through a somoned, oul and a the and the talling a part?\n",
      "\n",
      "Monica: Yeah, thes were and sore his a sofrent an thes to the sayon it some. The staress in any toning hore the there tho hard how to hore this a sare the live have werrabit.]\n",
      "\n",
      "Monica: You wat it it with the celches?)\n",
      "\n",
      "Phoebe: Oh, I don't gonna do and that we lide ano that is, to shut high.\n",
      "\n",
      "Rachel: Well, there's gotni goy.\n",
      "\n",
      "Chandler: What shing is to he sook.]\n",
      "\n",
      "Monich: Well, I'm toll now if at wher yer? We sires, y'know, it's now?\n",
      "\n",
      "Monica: (tastes to cheme out to store.)\n",
      "\n",
      "Caral: Y'know... That what and you guts to that to was out and werser, I just shake.. I know. (To Chandler) The can, I hand you seen whel you cand that hing, with the chears.\n",
      "\n",
      "Ross: Oh, I don't, I'd gonga hon her a tale to shouls and tolke a bat abaut?\n",
      "\n",
      "Ross: I mean, way it, ard you says think this was and that wame ab to day to stair.\n",
      "\n",
      "Rachel: I danna say it to a this to whis abous, ald about out it it sits that to do this, in home with him.\n",
      "\n",
      "Joey: Yeah alr the celling to look.\n",
      "\n",
      "Monica: (sparantered and Chandler..)\n",
      "\n",
      "Monica: I was to here.\n",
      "\n",
      "Rachel: (outtandertom) What a cantered allighs and seelly have all she hore a soment a that is? (He a pest ther if anly on of to and shoke the last ane the tarted hight, and thene's watt to this huntersiget.)\n",
      "\n",
      "Rachel: I'm torentedn hit they abeating whar is is womengering it it and somesten to ang wasted and your to tho kewan and some the comeders.]\n",
      "\n",
      "Monica: (theres are tats and sheet the give out.. think there's gotno think is whit is and thands in this a sophered.]\n",
      "\n",
      "Ross: (sacling and Carol ther ara sares.)\n",
      "\n",
      "Joey: Yeat, that sate ase.\n",
      "\n",
      "Mandia: (spitters and are heres there of the lithes her out at the shent.) I don't do to the know her the sincered.)\n",
      "\n",
      "Ross: (cumenich his to the chone.) Well, you know a saybige.\n",
      "\n",
      "Rachel: What as the stapting all abin thing of to have thing? Wan you sean you thit' going the hare this it so grones to thing all ther if werlis. We cout..... I wat a dide to hare a gand a carctes a there?\n",
      "\n",
      "Ross: Well you sell and any a littis.]\n",
      "\n",
      "Ross: I meam, I don't know intad the gives a chean a trees a to the soon the phamens the can and stalp an the chooke.)\n",
      "\n",
      "Monica: You wast a last,.\n",
      "\n",
      "Monica: Who want you' honeang and your. What sit hat thes an harly. You want to the look.) I well. We're that the door out wat and your.]\n",
      "\n",
      "Joey: Oh if well it, it a can the sary, and the siten at it so with how tome that some, the can.)\n",
      "\n",
      "Joey: Weal, whe's got with this tore\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 10000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
